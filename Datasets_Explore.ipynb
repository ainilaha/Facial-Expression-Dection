{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Introduction\n",
    "\n",
    "This notebook is highly depend on **P. Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar and I. Matthews, \"The Extended Cohn-Kanade Dataset (CK+): A complete dataset for action unit and emotion-specified expression,\" 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Workshops, San Francisco, CA, 2010, pp. 94-101.**\n",
    "\n",
    "### 1.1 CK vs CK+\n",
    "The [Cohn-Kanade AU-Coded Facial Expression Database](https://www.pitt.edu/~emotion/ck-spread.htm) is for research in automatic facial image analysis and synthesis and for perceptual studies. Cohn-Kanade is available in two versions and a third is in preparation.\n",
    "\n",
    "- The CK (or DFAT) database contains 486 sequences across 97 subjects.\n",
    "- Extended Cohn-Kanade (CK+) database added another 107 sequences as well as another 26 subjects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 The Extended Cohn-Kanade (CK+) Dataset\n",
    "\n",
    "Facial behavior of 210 adults was recorded using two hardware synchronized Panasonic AG-7500 cameras. Participants were 18 to 50 years of age, 69% female, 81%, Euro-American, 13% Afro-American, and 6% other groups. Participants were instructed by an experimenter to perform a series of 23 facial displays; these included single action units and combinations of action units.Each display began and ended in a neutral face with any exceptions noted. Im- age sequences for frontal views and 30-degree views were digitized into either 640x490 or 640x480 pixel arrays with 8- bit gray-scale or 24-bit color values\n",
    "\n",
    "For the CK+ distribution, we have augmented the dataset further to include 593 se- quences from 123 subjects (an additional 107 (22%) se- quences and 26 (27%) subjects). The image **sequence vary in duration (i.e. 10 to 60 frames)** and incorporate the onset (which is also the neutral frame) to peak formation of the facial expressions\n",
    "![examples](images/examples.jpg)\n",
    "\n",
    "All image data from the pool of **593 sequences** that had a nominal emotion label based on the subjectâ€™s impression of each of the **7 basic emotion categories: Anger,Contempt, Disgust, Fear, Happy, Sadness and Surprise.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Exploratory "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 AU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2  FACS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Emotional Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Base Line Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Active Appearance Model (AAM) \n",
    "### 3.2  Support Vector Machines (SVMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
